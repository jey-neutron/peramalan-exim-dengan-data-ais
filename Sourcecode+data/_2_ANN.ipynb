{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time                     #time\n",
    "import numpy as np              #tools for computing array\n",
    "import pandas as pd             #data manipulation\n",
    "import matplotlib.pyplot as plt #plot or graphic\n",
    "from sklearn.metrics import mean_squared_error #fungsi untuk menghitung mse rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Fungsi ANN\n",
    "Fungsi ANN akan dibangun dengan menggunakan referensi https://www.coursera.org/learn/neural-networks-deep-learning/lecture/znwiG/forward-and-backward-propagation\n",
    "\n",
    "Tahapan ANN dan beberapa fungsi tambahan dalam file ini dapat dilihat pada link berikut:\n",
    "- <a href='#initparam'>Inisialisasi parameter</a>\n",
    "- <a href='#fa'>Activation function</a>\n",
    "- <a href='#forward'>Forward propagation</a>\n",
    "- <a href='#cost'>Compute cost</a>\n",
    "- <a href='#backward'>Backward propagation</a>\n",
    "- <a href='#update'>Update parameters</a>\n",
    "- <a href='#ann'>Modelling ANN</a>\n",
    "- <a href='#predict'>Predict function</a>\n",
    "- <a href='#plot'>Plot function</a>\n",
    "\n",
    "Langkah pertama dalam ANN adalah inisialisasi parameter. Initialize_params merupakan fungsi untuk menginisialisasi parameter bobot dan bias sesuai dimensi layer dan neuron yang ditentukan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='initparam'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(layer_dims, jml_input, random_state=1):\n",
    "    '''\n",
    "    Inputs:\n",
    "    layer_dims -- size of hidden layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- dict of param (weight n bias)\n",
    "        Wi.shape = (n_i+1, n_i)\n",
    "        bi.shape = (n_i+1,1)\n",
    "    '''    \n",
    "    # nambah input dan output layer\n",
    "    layer_dims.insert(0,jml_input) #input\n",
    "    layer_dims.insert(len(layer_dims), 1) #out\n",
    "    \n",
    "    # init\n",
    "    if random_state: np.random.seed(random_state)\n",
    "    params = {}\n",
    "    n = len(layer_dims)\n",
    "    \n",
    "    for i in range(n-1):\n",
    "        # weight param\n",
    "        params['W'+str(i+1)] = np.random.randn(layer_dims[i+1], layer_dims[i])*0.1\n",
    "        # bias param\n",
    "        params['b'+str(i+1)] = np.zeros((layer_dims[i+1],1))\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi aktivasi akan dibangun dalam fungsi fa, dan akan dilengkapi dengan turunan fungsi aktivasi masing-masing. Fungsi aktivasi nantinya akan digunakan pada tahap forward propagation, sedangkan turunan fungsinya akan digunakan pada tahap backward propagation. Fungsi aktivasi yang dimaksud yaitu:\n",
    "- Tanh\n",
    "- Sigmoid\n",
    "- ReLU\n",
    "- Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fa'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fa(x, activation_func, derivative=False, alpha=0.01):\n",
    "    '''\n",
    "    Activation functions \n",
    "    Input:\n",
    "    x -- input for the function\n",
    "    activation_func -- type of func: tanh, sigmoid, relu, leaky relu\n",
    "    derivative -- if True, then x will calc with derivative func\n",
    "    alpha -- only used when activation func is 'leaky relu'\n",
    "    \n",
    "    Return:\n",
    "    y -- output of the func\n",
    "    '''\n",
    "    # ident\n",
    "    if activation_func == 'identity':\n",
    "        y = x if not derivative else 1\n",
    "    \n",
    "    # tanh\n",
    "    elif activation_func == 'tanh':\n",
    "        y = np.tanh(x) if not derivative else 1-np.power(np.tanh(x),2)\n",
    "    \n",
    "    # sigmoid\n",
    "    elif activation_func == 'sigmoid': \n",
    "        a = np.exp(-x)\n",
    "        sig = 1/(1+a)\n",
    "        y = sig if not derivative else sig*(1-sig)\n",
    "        \n",
    "    # relu\n",
    "    elif activation_func == 'relu':\n",
    "        y = np.maximum(0,x) if not derivative else np.where(x<0, 0, 1)\n",
    "            \n",
    "    # leaky relu\n",
    "    elif activation_func == 'leaky_relu':\n",
    "        y = np.maximum(alpha*x,x) if not derivative else np.where(x<0, alpha, 1)        \n",
    "    \n",
    "    # error karna selain fungsi di atas    \n",
    "    else: \n",
    "        raise Exception (f' \\'{activation_func}\\' function not found')\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tahap selanjutnya adalah forward propagation. Data input akan dikalikan dengan parameter bobot dan ditambah bias. Selanjutnya hasil tersebut akan diaktivasi dengan fungsi aktivasi. Proses ini terus berlanjut hingga mencapai layer output.\n",
    "\n",
    "Forward Propagation untuk layer $l$ dapat dituliskan:\n",
    "$$Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$$\n",
    "$$A^{[l] } = \\mbox{fa}^{[l]}(Z^{[l]})$$\n",
    "$$\\hat{Y} = A^{[n]} = \\mbox{fa}^{[n]}(Z^{ [n]})$$\n",
    "\n",
    "dimana Z adalah perkalian bobot dengan input/aktivasi layer sebelumnya ditambah bias; b adalah bias; A adalah output fungsi aktivasi (fa) dari Z; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='forward'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X,params, activation_func):\n",
    "    '''\n",
    "    Inputs:\n",
    "    X -- data X\n",
    "    params -- dict of params (from initialize_params)\n",
    "    activation_func -- activation function for hidden layer, \n",
    "                        output layer use identity (sementara)\n",
    "    \n",
    "    Returns:\n",
    "    AL -- output of last activation (in output layer)\n",
    "    cache -- dict containing Z, A\n",
    "    '''\n",
    "    cache = {}\n",
    "    cache['A0'] = X\n",
    "    \n",
    "    # calc Zi and Ai\n",
    "    # untuk setiap layer hidden n output\n",
    "    for i in range(round(len(params)/2)):\n",
    "        # Zi\n",
    "        cache['Z'+str(i+1)] = np.dot(params['W'+str(i+1)],cache['A'+str(i)]) +\\\n",
    "        params['b'+str(i+1)]\n",
    "        \n",
    "        # activation Zi -> Ai\n",
    "        cache['A'+str(i+1)] = fa(cache['Z'+str(i+1)], activation_func)\n",
    "    \n",
    "    # untuk output aj -> activation function pake identity\n",
    "    cache['A'+str(i+1)] = cache['Z'+str(i+1)]\n",
    "    #del cache['A0']\n",
    "    \n",
    "    return cache['A'+str(i+1)], cache #AL -> output prediksi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil forward propagation akan dihitung cost/loss dengan menghitung selisih nilai prediksi dan nilai sebenarnya.\n",
    "Cost $J$ pada $y^{(i)}$ tersebut dapat dihitung dengan: \n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small $$\n",
    "atau dengan SSE\n",
    "$$J = \\frac{1}{2} \\sum\\limits_{i}\\small\\left( y^{(i)}-\\mbox{fa} (z^{(i)}) \\small\\right)^{2}$$\n",
    "Atau bisa dengan menggunakan RMSE.\n",
    "Chunk di bawah menggunakan SSE untuk menghitung cost dari forward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cost'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    '''\n",
    "    Input:\n",
    "    AL -- output of last activation / prediction \n",
    "    Y -- true label / target data\n",
    "    \n",
    "    Return:\n",
    "    cost -- cross entropy cost \n",
    "    '''\n",
    "    m = Y.shape[1]\n",
    "    Y = Y.T.reshape(m,1)\n",
    "    AL = AL.T.reshape(m,1)\n",
    "    \n",
    "    # Calculate cost\n",
    "    #logprobs = np.multiply(np.log(AL), Y) + np.multiply(np.log(1-AL), 1-Y)\n",
    "    #cost = - np.sum(logprobs)/m\n",
    "    #cost = mean_squared_error(Y, AL, squared=False) #RMSE if squared is false\n",
    "    #cost = np.sqrt(np.average((Y-AL) ** 2, axis=0)) #RMSE\n",
    "    cost = np.average((Y-AL) ** 2, axis=0) / 2 #MSE\n",
    "    \n",
    "    # Clean\n",
    "    cost = float(np.squeeze(cost))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tahap selanjutnya adalah backward propagation. Error yang dihitung berdasarkan nilai target dan hasil prediksi akan dipropagasikan ke belakang dengan mengubah bobot dan biasnya. Backward propagation untuk layer $l$ dapat dihitung menggunakan rumus:\n",
    "$$dZ^{[l]} =  dA^{[l]}*\\mbox{fa}^{[l]'}(Z^{[l]})$$ \n",
    "$$dW^{[l]} = \\frac{1}{m}dZ^{[l]} . A^{[l-1]T}$$\n",
    "$$db^{[l]} = \\frac{1}{m}\\mbox{np.sum}(dZ^{[l]} \\mbox{, axis=1, keepdims=True})$$\n",
    "$$dA^{[l-1]} = W^{[l]T}.dZ^{[l]}$$\n",
    "Setelah mendapat hasil backward propagation, hasil tersebut digunakan untuk mengupdate parameter bobot dan bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='backward'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(params, cache, Y, activation_func):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    params -- dict containing params\n",
    "    cache -- dict containing Z and A\n",
    "    Y -- true labels\n",
    "    activation_func -- the activation func like in forward prop\n",
    "    \n",
    "    Return:\n",
    "    grads -- dict containing gradients with respect to different params\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    grads, temp = {},{}\n",
    "    l = round(len(params)/2)\n",
    "    \n",
    "    # Backward propagation: calc dWi, dbi \n",
    "    # utk setiap layer tapi dibalik\n",
    "    for i in reversed( range(1,l+1) ):\n",
    "        # dZ / delta error\n",
    "        # output\n",
    "        if (i == l):\n",
    "            ## dz = da * g'(z); da = dL/da\n",
    "            selisih = Y - cache['A'+str(i)]\n",
    "            temp['dZ'+str(i)] = -selisih # selisih / np.sqrt(\n",
    "                #np.average((Y-cache['A'+str(i)]) ** 2, axis=0))\n",
    "        # hidden\n",
    "        else:\n",
    "            temp['dZ'+str(i)] = \\\n",
    "                np.dot(params['W'+str(i+1)].T, temp['dZ'+str(i+1)])* \\\n",
    "                fa(cache['Z'+str(i)],activation_func,True)#(1 - np.power(A1, 2))\n",
    "        \n",
    "        # dW db\n",
    "        grads['dW'+str(i)] = np.dot(temp['dZ'+str(i)], cache['A'+str(i-1)].T)/m\n",
    "        grads['db'+str(i)] = np.sum(temp['dZ'+str(i)], axis=1, keepdims=True)/m\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='update'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, delta_prev, momentum, learning_rate):\n",
    "    '''\n",
    "    Input: \n",
    "    params -- dict of params\n",
    "    grads -- dict of gradients\n",
    "    delta_prev -- previous dict delta params for calculate momentum\n",
    "    momentum -- momentum rate param\n",
    "    learning_rate -- learning_rate param\n",
    "    \n",
    "    Return:\n",
    "    params -- updated params\n",
    "    delta_prev -- previous dict delta params updated\n",
    "    '''\n",
    "    # init\n",
    "    l = round(len(params)/2)\n",
    "    \n",
    "    for i in reversed( range(1,l+1) ):    \n",
    "        # previous delta params\n",
    "        ## jika delta_prev masi kosong, init delta_prev dengan 0 sesuai dimensi params\n",
    "        if not bool(delta_prev): \n",
    "            delta_prev['W'+str(i)] = params['W'+str(i)]*0\n",
    "            delta_prev['b'+str(i)] = params['b'+str(i)]*0\n",
    "        # update delta_prev\n",
    "        delta_prev['W'+str(i)] = learning_rate*grads['dW'+str(i)]\n",
    "        delta_prev['b'+str(i)] = learning_rate*grads['db'+str(i)]\n",
    "        # update params\n",
    "        params['W'+str(i)] -= momentum * delta_prev['W'+str(i)]\n",
    "        params['b'+str(i)] -= momentum * delta_prev['b'+str(i)] \n",
    "        \n",
    "    return params, delta_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi-fungsi di atas dijadikan satu fungsi sehingga membentuk suatu fungsi pemodelan ANN. Fungsi tersebut membutuhkan beberapa input seperti:\n",
    "- X,Y : data input dan targetnya\n",
    "- layer_dims : berisi jumlah hidden layer beserta jumlah neuronnya dalam bentuk list\n",
    "- learning_rate : parameter learning rate untuk mengupdate gradient descent. Nilai default akan diset dengan nilai 0.001\n",
    "- epoch : banyaknya iterasi pembelajaran. Nilai default akan diset sebanyak 1000 pengulangan\n",
    "- print_cost : apakah fungsi ANN perlu menampilkan hasil cost atau tidak, jika ya, maka akan menampilkan cost setiap 100 atau 10 perulangan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ann'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, layer_dims, activation_func, \n",
    "             momentum=0.9,learning_rate=0.001, epoch=1000,\n",
    "             print_cost=False, random_state=1, early_stop=False, param=None):\n",
    "    '''\n",
    "    Inputs:\n",
    "    X,Y -- data\n",
    "    layer_dims -- list containing the input size n each layer size\n",
    "    momentum -- momentum rate for gradient descent update\n",
    "    learning_rate -- learning rate of gradient descent update rule\n",
    "    epoch -- num of loop/num iteration\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    random_state -- random seed \n",
    "    early_stop -- jika True, maka loop akan dihentikan bila gada perbedaan cost\n",
    "                sebelumnya dengan cost selanjutnya\n",
    "    (param -- param weight dan bias)###########################################\n",
    "    \n",
    "    Returns:\n",
    "    params -- params learnt by model\n",
    "    AL -- prediction result\n",
    "    cost -- nilai cost / loss yang dihasilkan; selisih pred dan true\n",
    "    '''\n",
    "    # init n sesuaiin x,y\n",
    "    X = X.T\n",
    "    Y = Y.T.reshape(1,Y.shape[0])\n",
    "    cost = np.zeros(Y.shape)\n",
    "    costs = []\n",
    "    delta_prev={}\n",
    "    \n",
    "    # init param weight n bias\n",
    "    # jika gada param weight dan bias yang dimasukin, berarti random aja\n",
    "    if param is None:\n",
    "        params = initialize_params(layer_dims, jml_input=X.shape[0], \n",
    "                                   random_state=random_state)\n",
    "    else:\n",
    "        raise Exception('Belum di set wkwkwk; Tar akan disesuaiin')\n",
    "    \n",
    "    # loop (gradient descent)\n",
    "    for i in range(0, epoch):\n",
    "        # forward\n",
    "        AL, caches = forward_propagation(X, params, activation_func)\n",
    "        # cost\n",
    "        ## jika dikasi early stop\n",
    "        if early_stop:\n",
    "            ## hitung selisih nilai cost min sebelumnya dg sekarang\n",
    "            dcost_min = np.min(np.abs(cost-compute_cost(AL,Y)))\n",
    "            ## jika kurang dari threshold maka hentikan\n",
    "            if dcost_min <= 1e-4 :\n",
    "                if print_cost:\n",
    "                    print(dcost_min)\n",
    "                    print(f'Iterasi dihentikan pada {i} dari {epoch} karena tidak ada perubahan cost')\n",
    "                break\n",
    "        cost = compute_cost(AL,Y)\n",
    "        # back\n",
    "        grads = backward_propagation(params, caches, Y, activation_func)\n",
    "        # update params\n",
    "        params, delta_prev = update_params(params, grads, delta_prev, momentum, learning_rate)\n",
    "        \n",
    "        # Print the cost every 100 or 10 training example\n",
    "        bil = 10 if epoch<=100 else 100\n",
    "        if print_cost and i % bil == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % bil == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    if print_cost:\n",
    "        plot([costs], xlabel='iterations (per hundreds)', ylabel='cost',\n",
    "             title=\"Cost Graph with learning rate =\" + str(learning_rate))\n",
    "        \n",
    "    t = 1000 * time.time() # current time in milliseconds\n",
    "    np.random.seed(int(t) % 2**32)\n",
    "    \n",
    "    return params, np.squeeze(AL.T), cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi predict digunakan untuk memprediksi data target berdasarkan data input. Fungsi akan melakukan forward propagation berdasarkan parameter bobot dan bias yang terbentuk (yang berasal dari tahapan ANN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='predict'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, params, activation_func):\n",
    "    '''\n",
    "    Input:\n",
    "    params -- dict of params\n",
    "    X -- input data\n",
    "    \n",
    "    Return:\n",
    "    AL -- prediction\n",
    "    '''\n",
    "    X = X.T\n",
    "    \n",
    "    AL, cache = forward_propagation(X,params, activation_func)\n",
    "    #predictions = (AL > 0.5)\n",
    "    \n",
    "    return np.squeeze(AL.T)#,predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi membuat plot / grafik -> agar support dark mode sih\n",
    "def plot(x, y=None, label=None, title=None, \n",
    "         xlabel=None, ylabel=None, xticks=None, color='grey'):\n",
    "    '''\n",
    "    Plot x and y with pyplot\n",
    "    Input:\n",
    "    x -- data x di x axis, dapat > 1\n",
    "    y -- data y di y axis (optional), dapat > 1\n",
    "    label -- label data (khususnya jika x lebih dari 1)\n",
    "    title -- judul grafik\n",
    "    xlabel, ylabel -- label axis x dan y\n",
    "    xticks -- list of x axis ticks\n",
    "    color -- warna text grafik\n",
    "    '''\n",
    "    # init\n",
    "    label2 = np.resize( np.array([None]),len(x)) if label is None else label\n",
    "    plt.figure(figsize=(10,3))\n",
    "\n",
    "    # plot per data\n",
    "    if y is None:\n",
    "        for i,lab in zip(x,label2): plt.plot(np.squeeze(i), label=lab)\n",
    "    else:\n",
    "        for i,j,lab in zip(x,y,label2): \n",
    "            plt.plot(np.squeeze(i),np.squeeze(j), label=lab)\n",
    "    #plt.plot(x) if y is None else plt.plot(x,y)\n",
    "    \n",
    "    # label xy\n",
    "    plt.ylabel(ylabel, color=color)\n",
    "    plt.yticks(color=color)\n",
    "    plt.xlabel(xlabel, color=color)\n",
    "    if xticks is not None:\n",
    "        plt.xticks([i for i in range(len(xticks))], xticks, color=color)\n",
    "    else :\n",
    "        plt.xticks(color=color)\n",
    "    plt.title(title, color=color)\n",
    "    \n",
    "    # show\n",
    "    if label is not None: plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fungsi pelengkap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_param(gen):\n",
    "    '''\n",
    "    Konversi nilai gen biar lebih manusiawi\n",
    "    \n",
    "    Input: \n",
    "    gen -- list nilai gen yg berupa bil\n",
    "    \n",
    "    Return:\n",
    "    param -- dict berisi konversi nilai gen dari bil. ke ...tipenya (kali ya)\n",
    "    '''\n",
    "    param = {}\n",
    "    \n",
    "    # gen 1-4 -> hidden layer size n neuron size\n",
    "    hidden_param = []\n",
    "    for i in range(0,4): \n",
    "        # jika nilai gen tsb 0 ato negatif, maka berarti gada layer di urutan tsb dan selanjutnya\n",
    "        if round(gen[i]) <= 0 : break\n",
    "        # bulatin nilainya ke integer\n",
    "        hidden_param.append(round(gen[i]))\n",
    "    param['hidden'] = hidden_param\n",
    "    \n",
    "    # the rest gen konversi ke masing-masing tipe\n",
    "    param['activation'] = ['identity','tanh', 'sigmoid','relu','leaky_relu'][int( np.floor(gen[4]) )]\n",
    "    param['learning_rate'] = gen[5]\n",
    "    param['momentum'] = gen[6]\n",
    "    \n",
    "    return param\n",
    "\n",
    "def mape(y_true, y_pred): \n",
    "    '''Fungsi MAPE'''\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implemen data dummy\n",
    "Data dibawah merupakan hasil uji coba ANN menggunakan data dummy."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# INIT PARAMS YANG DIPERLUKAN\n",
    "learning_rate = 0.001\n",
    "activation = 'relu'\n",
    "layer_dims = [100]\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# dataset\n",
    "dfx = pd.DataFrame({\n",
    "    'a':[1,3,4,5,7,9],\n",
    "    'b':[5,3,6,8,5,3],\n",
    "    'c':[4,5,6,5,4,5],\n",
    "    'y':[1,2,3,4,5,6]\n",
    "})\n",
    "dfx = pd.DataFrame({\n",
    "    'a':[1.1,3.3,4.4,5.5,7.6,9.3],\n",
    "    'b':[5.4,3.6,6.6,8.2,5.56,3.6],\n",
    "    'c':[4.0,5.0,6.0,5.0,4.0,5.0],\n",
    "    'y':[1.1,2.2,3.3,4.4,5.5,6.6]\n",
    "})\n",
    "X = dfx.iloc[:,:3].to_numpy()\n",
    "Y = dfx.iloc[:,3].to_numpy()\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nn_model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# modelling\n",
    "params,pred,best_cost = nn_model(X,Y,layer_dims, \n",
    "                                 activation_func = activation, \n",
    "                                 learning_rate = learning_rate, \n",
    "                                 momentum=momentum,\n",
    "                                 epoch=1000,print_cost=True, \n",
    "                                 random_state=1,\n",
    "                                 early_stop=False)\n",
    "#params"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# hasil predict\n",
    "print(best_cost)\n",
    "print(np.squeeze(X))\n",
    "print(Y)\n",
    "predict(X, params, activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perbandingan dengan sklearn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "regr = MLPRegressor()\n",
    "regr.fit(X,Y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "regr.predict(X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "regr.loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "220px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
